<!DOCTYPE html>
<html lang="en">

<head>
  <br>
  <!-- Basic Page Needs
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>CAP</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/footable.standalone.min.css">

  <!-- Favicon
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon"  href="./img/logo.svg">

  <!-- Google icon -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0" />

  <!-- Hover effect: https://codepen.io/nxworld/pen/ZYNOBZ -->
  <style>
    img {
      display: block;
    }

    .column-50 {
      float: left;
      width: 50%;
    }

    .row-50:after {
      content: "";
      display: table;
      clear: both;
    }

    .floating-teaser {
      float: left;
      width: 30%;
      text-align: center;
      padding: 15px;
    }

    .venue strong {
      color: #99324b;
    }

    .benchmark {
      width: 100%;
      max-width: 960px;
      overflow: scroll;
      overflow-y: hidden;
    }
  </style>
</head>

<body>

  <!-- Primary Page Layout
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <h4 style="text-align:center">CompUDA: Compositional Unsupervised Domain Adaptation for Semantic Segmentation under Adverse Conditions</h4>
    <p align="center" , style="margin-bottom:12px;">
      <a class="simple" href="#" target="_blank">Ziqiang Zheng</a><sup>1</sup>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a class="simple" href="https://chenyingshu.github.io/" target="_blank">Yingshu Chen</a><sup>1</sup>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a class="simple" href="https://sonhua.github.io/" target="_blank">Binh-Son Hua</a><sup>2,3</sup>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a class="simple" href="#">Yang Wu</a><sup>4</sup>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a class="simple" href="https://saikit.org/" target="_blank">Sai-Kit Yeung</a><sup>1</sup>
    </p>

    <p align="center" style="margin-bottom:20px;">
      <sup>1</sup>The Hong Kong University of Science and Technology
      <br>
      <sup>2</sup>Trinity College Dublin
      <br>
      <sup>3</sup>VinAI Research
      <br>
      <sup>4</sup>Tencent AI Lab
    </p>

    <div class="venue">
      <p align="center"><b>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2023</b></p>
    </div>

  
    <!-- Framework -->
    <figure>
      <img src="img/framework.png" style="width:100%"></img>
    </figure>
    
    <p align="justify"><b>Framework Overview.</b> The core of our image-level domain adaptation is a cycle of three mutual-beneficial modules: cross-domain content alignment (CDCA), reference-guided image synthesis and contrastive learning. CDCA uses the domain-invariant feature extractor learned by contrastive learning to construct source-reference image pairs for training the reference-guided image synthesis module to produce target-like images. The source and target-like images can be regarded as augmented views for contrastive learning to improve the domain-invariant feature extractor. Final target-like images and source labels can be adopted for downstream perception tasks. </p>

    <div id="teaser" class="container" style="width:100%; margin:0; padding:0">

      <h5>Abstract</h5>
      <p align="justify">
          Addressing domain shifts for complex perception tasks in autonomous driving has long been a challenging problem.
          In this paper, we show that existing domain adaptation methods pay little attention to the content mismatch issue between source and target domains, thus weakening the domain adaptation performance and the decoupling of domain-invariant and domainspecific representations. To solve the aforementioned problems, we propose an image-level domain adaptation framework that aims at adapting source-domain images to the target domain with content-aligned source-target image pairs. Our framework consists of three mutually beneficial modules in a cycle: a cross-domain content alignment module to generate source-target pairs with consistent content representations in a selfsupervised manner, a reference-guided image synthesis based on the generated content-aligned source-target image pairs, and a contrastive learning module to self-supervise domain-invariant feature extractor. Our contrastive appearance adaptation is task agnostic and robust to complex perception tasks in autonomous driving. Our proposed method demonstrates state-of-the-art results in cross-domain object detection, semantic segmentation, and depth estimation as well as better image synthesis ability qualitatively and quantitatively.
        <br>
        <br>
      </p>
    </div>



    <div class="section">
      <h5>Materials</h5>
      <div class="container" style="width:95%">
        <div class="row">
          <div class="three columns" style="display: flex; flex-direction: column; align-items: center;">
            <a href="./assets/iros2023_CAP.pdf" target="_blank">
              <span style="border: 1px solid #ddd; border-radius: 4px; padding: 2px; font-size: 108px;" class="material-symbols-outlined">
              article
              </span>
            </a>
            <a href="./assets/iros2023_CAP.pdf" target="_blank">Paper</a>
          </div>
          <!-- <div class="three columns" style="display: flex; flex-direction: column; align-items: center;">
            <a href="#" target="_blank">
              <span style="border: 1px solid #ddd; border-radius: 4px; padding: 2px; font-size: 108px;" class="material-symbols-outlined">
              code
              </span></a>
              <a href="#" target="_blank">Code </a>
          </div> -->
        </div>
      </div>
    </div> 


    <br>

    


    <div class="section">
      <h5>Citation</h5>
      <pre style="margin:0">
        <code>@inproceedings{zheng2023compuda,
          title={CompUDA: Compositional Unsupervised Domain Adaptation for Semantic Segmentation under Adverse Conditions},
          author={Ziqiang Zheng and Yingshu Chen and Binh-Son Hua and Sai-Kit Yeung},
          booktitle={2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
          year={2023}
          organization={IEEE}
        }</code>
        </pre>
    </div>

    <!-- -->
    <br>

  <!-- End Document
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>

</html>
